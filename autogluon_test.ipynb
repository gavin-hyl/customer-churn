{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "# import autogluon.tabular as agtb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20240209_054805\"\n",
      "Presets specified: ['high_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 3600 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: AutogluonModels\\ag-20240209_054805/ds_sub_fit/sub_fit_ho.\n",
      "2024-02-08 21:48:06,406\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Beginning AutoGluon training ... Time limit = 900s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240209_054805/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          32\n",
      "Memory Avail:       21.60 GB / 31.75 GB (68.0%)\n",
      "Disk Space Avail:   140.28 GB / 276.64 GB (50.7%)\n",
      "===================================================\n",
      "Train Data Rows:    4749\n",
      "Train Data Columns: 19\n",
      "Label Column:       Discontinued\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    22121.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.43 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  2 | ['MonthlyCharges', 'TotalCharges']\n",
      "\t\t('int', [])    :  2 | ['SeniorCitizen', 'tenure']\n",
      "\t\t('object', []) : 15 | ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', ...]\n",
      "\t\t('float', [])     :  2 | ['MonthlyCharges', 'TotalCharges']\n",
      "\t\t('int', [])       :  1 | ['tenure']\n",
      "\t\t('int', ['bool']) :  6 | ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.19 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.18s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 599.73s of the 899.8s of remaining time.\n",
      "\t-0.4205\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 594.68s of the 894.75s of remaining time.\n",
      "\t-0.4331\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 594.62s of the 894.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3686\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 571.08s of the 871.15s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3688\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 565.34s of the 865.41s of remaining time.\n",
      "\t-0.3875\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 563.57s of the 863.65s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3662\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.58s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 513.66s of the 813.74s of remaining time.\n",
      "\t-0.3898\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.05s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 512.17s of the 812.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.371\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.55s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 490.93s of the 791.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.05%)\n",
      "\t-0.3716\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 485.26s of the 785.34s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4122\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.62s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 461.99s of the 762.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.08%)\n",
      "\t-0.3763\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 454.55s of the 754.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.367\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 448.16s of the 748.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4284\t = Validation score   (-root_mean_squared_error)\n",
      "\t40.06s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 403.68s of the 703.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3704\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 396.78s of the 696.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3701\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.01s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 359.1s of the 659.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.15%)\n",
      "\t-0.3685\t = Validation score   (-root_mean_squared_error)\n",
      "\t29.14s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 325.2s of the 625.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3673\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.89s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 318.97s of the 619.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4579\t = Validation score   (-root_mean_squared_error)\n",
      "\t86.53s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 228.12s of the 528.2s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.29%)\n",
      "\t-0.3788\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 221.04s of the 521.11s of remaining time.\n",
      "\t-0.3899\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 219.36s of the 519.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3664\t = Validation score   (-root_mean_squared_error)\n",
      "\t16.49s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 198.51s of the 498.58s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3694\t = Validation score   (-root_mean_squared_error)\n",
      "\t7.08s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 187.07s of the 487.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.16%)\n",
      "\t-0.3679\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.93s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 177.43s of the 477.5s of remaining time.\n",
      "\t-0.3856\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 175.6s of the 475.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.07%)\n",
      "\t-0.3722\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 168.55s of the 468.62s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3695\t = Validation score   (-root_mean_squared_error)\n",
      "\t33.24s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 130.78s of the 430.85s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.05%)\n",
      "\t-0.3683\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 125.06s of the 425.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.482\t = Validation score   (-root_mean_squared_error)\n",
      "\t99.18s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 21.4s of the 321.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.06%)\n",
      "\t-0.368\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 14.87s of the 314.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.513\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.52s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 295.44s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.395, 'NeuralNetFastAI_r191_BAG_L1': 0.158, 'NeuralNetTorch_r79_BAG_L1': 0.132, 'LightGBM_r130_BAG_L1': 0.132, 'LightGBM_BAG_L1': 0.105, 'NeuralNetFastAI_BAG_L1': 0.079}\n",
      "\t-0.3633\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 294.57s of the 294.28s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.07%)\n",
      "\t-0.3653\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 288.19s of the 287.89s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.07%)\n",
      "\t-0.3664\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 281.82s of the 281.53s of remaining time.\n",
      "\t-0.3687\t = Validation score   (-root_mean_squared_error)\n",
      "\t5.59s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 275.74s of the 275.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.11%)\n",
      "\t-0.3633\t = Validation score   (-root_mean_squared_error)\n",
      "\t60.09s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 210.9s of the 210.61s of remaining time.\n",
      "\t-0.366\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 209.38s of the 209.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.367\t = Validation score   (-root_mean_squared_error)\n",
      "\t15.57s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 189.48s of the 189.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.13%)\n",
      "\t-0.3671\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 180.83s of the 180.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3994\t = Validation score   (-root_mean_squared_error)\n",
      "\t14.48s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 161.93s of the 161.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.22%)\n",
      "\t-0.3701\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.49s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 146.94s of the 146.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.11%)\n",
      "\t-0.3635\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.05s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 132.48s of the 132.18s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.4303\t = Validation score   (-root_mean_squared_error)\n",
      "\t27.48s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L2 ... Training model for up to 100.64s of the 100.35s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.10%)\n",
      "\t-0.367\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.33s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2 ... Training model for up to 91.98s of the 91.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3682\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.77s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L2 ... Training model for up to 56.88s of the 56.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.38%)\n",
      "\t-0.364\t = Validation score   (-root_mean_squared_error)\n",
      "\t45.51s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 6.25s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_r177_BAG_L2': 0.212, 'ExtraTreesMSE_BAG_L2': 0.192, 'XGBoost_BAG_L2': 0.173, 'NeuralNetFastAI_BAG_L2': 0.135, 'CatBoost_BAG_L1': 0.096, 'CatBoost_BAG_L2': 0.077, 'NeuralNetFastAI_r191_BAG_L2': 0.077, 'NeuralNetFastAI_r191_BAG_L1': 0.038}\n",
      "\t-0.3616\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 894.04s ... Best model: \"WeightedEnsemble_L3\"\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting model: KNeighborsUnif_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMXT_BAG_L1_FULL ...\n",
      "\t0.27s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\t0.14s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\t4.12s\t = Training   runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.05s\t = Training   runtime\n",
      "\t0.33s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L1_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 7.\n",
      "\t1.31s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_BAG_L1_FULL ...\n",
      "\t0.07s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t1.2s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L1_FULL ...\n",
      "\t0.26s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_r177_BAG_L1_FULL ...\n",
      "\t0.15s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1_FULL ...\n",
      "\t4.68s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_r131_BAG_L1_FULL ...\n",
      "\t0.33s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 23.\n",
      "\t5.63s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_r9_BAG_L1_FULL ...\n",
      "\t2.27s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_r96_BAG_L1_FULL ...\n",
      "\t0.3s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1_FULL ...\n",
      "\t10.51s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_r33_BAG_L1_FULL ...\n",
      "\t0.25s\t = Training   runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_r137_BAG_L1_FULL ...\n",
      "\t2.1s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 14.\n",
      "\t0.78s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: CatBoost_r13_BAG_L1_FULL ...\n",
      "\t0.62s\t = Training   runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1_FULL | Skipping fit via cloning parent ...\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_r188_BAG_L1_FULL ...\n",
      "\t0.27s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 16.\n",
      "\t6.23s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: XGBoost_r89_BAG_L1_FULL ...\n",
      "\t0.05s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1_FULL ...\n",
      "\t9.9s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: LightGBM_r130_BAG_L1_FULL ...\n",
      "\t0.15s\t = Training   runtime\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1_FULL ...\n",
      "\t2.08s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.395, 'NeuralNetFastAI_r191_BAG_L1': 0.158, 'NeuralNetTorch_r79_BAG_L1': 0.132, 'LightGBM_r130_BAG_L1': 0.132, 'LightGBM_BAG_L1': 0.105, 'NeuralNetFastAI_BAG_L1': 0.079}\n",
      "\t0.82s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2_FULL ...\n",
      "\t0.19s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "\t0.15s\t = Training   runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t5.59s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\t4.78s\t = Training   runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2_FULL | Skipping fit via cloning parent ...\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 6.\n",
      "\t1.27s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: XGBoost_BAG_L2_FULL ...\n",
      "\t0.12s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\t0.35s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: LightGBMLarge_BAG_L2_FULL ...\n",
      "\t0.41s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: CatBoost_r177_BAG_L2_FULL ...\n",
      "\t0.56s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L2_FULL ...\n",
      "\t2.18s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: LightGBM_r131_BAG_L2_FULL ...\n",
      "\t0.53s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L2_FULL ...\n",
      "d:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\models\\fastainn\\tabular_nn_fastai.py:200: FutureWarning: The 'downcast' keyword in fillna is deprecated and will be removed in a future version. Use res.infer_objects(copy=False) to infer non-object dtype, or pd.to_numeric with the 'downcast' keyword to downcast numeric results.\n",
      "  df = df.fillna(column_fills, inplace=False, downcast=False)\n",
      "\tStopping at the best epoch learned earlier - 4.\n",
      "\t1.34s\t = Training   runtime\n",
      "Fitting 1 L2 models ...\n",
      "Fitting model: CatBoost_r9_BAG_L2_FULL ...\n",
      "\t8.15s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoost_r177_BAG_L2': 0.212, 'ExtraTreesMSE_BAG_L2': 0.192, 'XGBoost_BAG_L2': 0.173, 'NeuralNetFastAI_BAG_L2': 0.135, 'CatBoost_BAG_L1': 0.096, 'CatBoost_BAG_L2': 0.077, 'NeuralNetFastAI_r191_BAG_L2': 0.077, 'NeuralNetFastAI_r191_BAG_L1': 0.038}\n",
      "\t0.27s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L3_FULL\" (Previously \"WeightedEnsemble_L3\"). AutoGluon will default to using \"WeightedEnsemble_L3_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 78.07s ... Best model: \"WeightedEnsemble_L3_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20240209_054805/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                               model  holdout_score  score_val              eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0          CatBoost_r137_BAG_L1_FULL      -0.356117  -0.366429  root_mean_squared_error        0.010993            NaN   2.095382                 0.010993                     NaN           2.095382            1       True         21\n",
      "1               CatBoost_BAG_L1_FULL      -0.356231  -0.366170  root_mean_squared_error        0.015529            NaN   4.121390                 0.015529                     NaN           4.121390            1       True          6\n",
      "2           LightGBM_r96_BAG_L1_FULL      -0.356308  -0.367287  root_mean_squared_error        0.017000            NaN   0.304148                 0.017000                     NaN           0.304148            1       True         17\n",
      "3             LightGBMXT_BAG_L1_FULL      -0.356382  -0.368557  root_mean_squared_error        0.018990            NaN   0.270348                 0.018990                     NaN           0.270348            1       True          3\n",
      "4            XGBoost_r89_BAG_L1_FULL      -0.356955  -0.368256  root_mean_squared_error        0.027001            NaN   0.045830                 0.027001                     NaN           0.045830            1       True         27\n",
      "5           CatBoost_r13_BAG_L1_FULL      -0.357354  -0.367864  root_mean_squared_error        0.009685            NaN   0.623030                 0.009685                     NaN           0.623030            1       True         23\n",
      "6           WeightedEnsemble_L3_FULL      -0.357477  -0.361562  root_mean_squared_error        1.348212            NaN  68.509185                 0.010999                     NaN           0.267197            3       True         46\n",
      "7           WeightedEnsemble_L2_FULL      -0.357546  -0.363336  root_mean_squared_error        0.177171            NaN  16.858074                 0.010000                     NaN           0.821465            2       True         31\n",
      "8               CatBoost_BAG_L2_FULL      -0.357989  -0.363340  root_mean_squared_error        1.111539            NaN  63.817333                 0.020998                     NaN           4.781000            2       True         35\n",
      "9          CatBoost_r177_BAG_L1_FULL      -0.358045  -0.367016  root_mean_squared_error        0.022507            NaN   0.147361                 0.022507                     NaN           0.147361            1       True         12\n",
      "10         LightGBM_r131_BAG_L1_FULL      -0.358147  -0.370419  root_mean_squared_error        0.022002            NaN   0.334189                 0.022002                     NaN           0.334189            1       True         14\n",
      "11         LightGBM_r130_BAG_L1_FULL      -0.358167  -0.367977  root_mean_squared_error        0.016001            NaN   0.154231                 0.016001                     NaN           0.154231            1       True         29\n",
      "12         ExtraTreesMSE_BAG_L2_FULL      -0.358724  -0.365974  root_mean_squared_error        1.166209            NaN  60.171434                 0.075669                0.295615           1.135101            2       True         36\n",
      "13  NeuralNetFastAI_r145_BAG_L1_FULL      -0.358792  -0.369521  root_mean_squared_error        0.052544            NaN   6.234540                 0.052544                     NaN           6.234540            1       True         26\n",
      "14            LightGBMXT_BAG_L2_FULL      -0.358849  -0.365285  root_mean_squared_error        1.112545            NaN  59.221338                 0.022004                     NaN           0.185005            2       True         32\n",
      "15  NeuralNetFastAI_r191_BAG_L1_FULL      -0.358937  -0.370097  root_mean_squared_error        0.051511            NaN   5.632577                 0.051511                     NaN           5.632577            1       True         15\n",
      "16               XGBoost_BAG_L1_FULL      -0.358978  -0.371579  root_mean_squared_error        0.016998            NaN   0.067357                 0.016998                     NaN           0.067357            1       True          9\n",
      "17              LightGBM_BAG_L1_FULL      -0.359647  -0.368838  root_mean_squared_error        0.010999            NaN   0.140558                 0.010999                     NaN           0.140558            1       True          4\n",
      "18       RandomForestMSE_BAG_L2_FULL      -0.359840  -0.368740  root_mean_squared_error        1.211055            NaN  64.621605                 0.120514                0.372908           5.585272            2       True         34\n",
      "19           CatBoost_r9_BAG_L2_FULL      -0.359999  -0.363987  root_mean_squared_error        1.121541            NaN  67.191166                 0.031000                     NaN           8.154833            2       True         45\n",
      "20         LightGBM_r188_BAG_L1_FULL      -0.360184  -0.372207  root_mean_squared_error        0.015682            NaN   0.272045                 0.015682                     NaN           0.272045            1       True         25\n",
      "21         CatBoost_r177_BAG_L2_FULL      -0.360247  -0.363531  root_mean_squared_error        1.111540            NaN  59.594620                 0.020999                     NaN           0.558286            2       True         41\n",
      "22           CatBoost_r9_BAG_L1_FULL      -0.360811  -0.368452  root_mean_squared_error        0.019000            NaN   2.273602                 0.019000                     NaN           2.273602            1       True         16\n",
      "23              LightGBM_BAG_L2_FULL      -0.361329  -0.366367  root_mean_squared_error        1.111049            NaN  59.187359                 0.020509                     NaN           0.151026            2       True         33\n",
      "24     RandomForest_r195_BAG_L1_FULL      -0.362026  -0.385648  root_mean_squared_error        0.068277       0.232246   1.481880                 0.068277                0.232246           1.481880            1       True         24\n",
      "25         LightGBMLarge_BAG_L1_FULL      -0.362292  -0.376317  root_mean_squared_error        0.016001            NaN   0.261508                 0.016001                     NaN           0.261508            1       True         11\n",
      "26         LightGBM_r131_BAG_L2_FULL      -0.362859  -0.367039  root_mean_squared_error        1.114541            NaN  59.563353                 0.024000                     NaN           0.527020            2       True         43\n",
      "27               XGBoost_BAG_L2_FULL      -0.363122  -0.367069  root_mean_squared_error        1.118054            NaN  59.154579                 0.027514                     NaN           0.118246            2       True         38\n",
      "28  NeuralNetFastAI_r102_BAG_L1_FULL      -0.363350  -0.369438  root_mean_squared_error        0.023087            NaN   0.776776                 0.023087                     NaN           0.776776            1       True         22\n",
      "29           XGBoost_r33_BAG_L1_FULL      -0.363490  -0.378769  root_mean_squared_error        0.038515            NaN   0.254578                 0.038515                     NaN           0.254578            1       True         19\n",
      "30       RandomForestMSE_BAG_L1_FULL      -0.364180  -0.387477  root_mean_squared_error        0.142520       0.204287   1.471572                 0.142520                0.204287           1.471572            1       True          5\n",
      "31       NeuralNetFastAI_BAG_L1_FULL      -0.364209  -0.370986  root_mean_squared_error        0.038127            NaN   1.306882                 0.038127                     NaN           1.306882            1       True          8\n",
      "32        ExtraTrees_r42_BAG_L1_FULL      -0.365751  -0.389921  root_mean_squared_error        0.148520       0.222803   1.328103                 0.148520                0.222803           1.328103            1       True         20\n",
      "33         LightGBMLarge_BAG_L2_FULL      -0.366710  -0.370125  root_mean_squared_error        1.110542            NaN  59.445071                 0.020001                     NaN           0.408737            2       True         40\n",
      "34         ExtraTreesMSE_BAG_L1_FULL      -0.366910  -0.389820  root_mean_squared_error        0.073003       0.334546   1.045338                 0.073003                0.334546           1.045338            1       True          7\n",
      "35       NeuralNetFastAI_BAG_L2_FULL      -0.370311  -0.367043  root_mean_squared_error        1.134522            NaN  60.306732                 0.043981                     NaN           1.270399            2       True         37\n",
      "36  NeuralNetFastAI_r191_BAG_L2_FULL      -0.374361  -0.368171  root_mean_squared_error        1.148052            NaN  60.378957                 0.057511                     NaN           1.342623            2       True         44\n",
      "37        KNeighborsUnif_BAG_L1_FULL      -0.396449  -0.420460  root_mean_squared_error        0.024639       0.033940   0.008940                 0.024639                0.033940           0.008940            1       True          1\n",
      "38        NeuralNetTorch_BAG_L2_FULL      -0.401732  -0.399392  root_mean_squared_error        1.126541            NaN  59.389863                 0.036000                     NaN           0.353530            2       True         39\n",
      "39        KNeighborsDist_BAG_L1_FULL      -0.405543  -0.433057  root_mean_squared_error        0.019897       0.028069   0.006008                 0.019897                0.028069           0.006008            1       True          2\n",
      "40    NeuralNetTorch_r79_BAG_L2_FULL      -0.426042  -0.430323  root_mean_squared_error        1.145053            NaN  61.217161                 0.054513                     NaN           2.180828            2       True         42\n",
      "41        NeuralNetTorch_BAG_L1_FULL      -0.431073  -0.412159  root_mean_squared_error        0.025001            NaN   1.196761                 0.025001                     NaN           1.196761            1       True         10\n",
      "42    NeuralNetTorch_r79_BAG_L1_FULL      -0.442163  -0.428362  root_mean_squared_error        0.035004            NaN   4.680971                 0.035004                     NaN           4.680971            1       True         13\n",
      "43    NeuralNetTorch_r22_BAG_L1_FULL      -0.445441  -0.457922  root_mean_squared_error        0.031001            NaN  10.513824                 0.031001                     NaN          10.513824            1       True         18\n",
      "44    NeuralNetTorch_r30_BAG_L1_FULL      -0.469463  -0.482001  root_mean_squared_error        0.052511            NaN   9.904162                 0.052511                     NaN           9.904162            1       True         28\n",
      "45    NeuralNetTorch_r86_BAG_L1_FULL      -0.507519  -0.513043  root_mean_squared_error        0.027999            NaN   2.082444                 0.027999                     NaN           2.082444            1       True         30\n",
      "Stacked overfitting occurred: False.\n",
      "Spend 975 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 2625 seconds.\n",
      "Starting full fit now with num_stack_levels 1.\n",
      "Beginning AutoGluon training ... Time limit = 2625s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20240209_054805\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22631\n",
      "CPU Count:          32\n",
      "Memory Avail:       19.92 GB / 31.75 GB (62.7%)\n",
      "Disk Space Avail:   140.28 GB / 276.64 GB (50.7%)\n",
      "===================================================\n",
      "Train Data Rows:    5343\n",
      "Train Data Columns: 19\n",
      "Label Column:       Discontinued\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    20402.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 4.99 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 6 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  :  2 | ['MonthlyCharges', 'TotalCharges']\n",
      "\t\t('int', [])    :  2 | ['SeniorCitizen', 'tenure']\n",
      "\t\t('object', []) : 15 | ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 10 | ['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', ...]\n",
      "\t\t('float', [])     :  2 | ['MonthlyCharges', 'TotalCharges']\n",
      "\t\t('int', [])       :  1 | ['tenure']\n",
      "\t\t('int', ['bool']) :  6 | ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t19 features in original data used to generate 19 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.21 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 1749.49s of the 2624.88s of remaining time.\n",
      "\t-0.4171\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 1749.42s of the 2624.82s of remaining time.\n",
      "\t-0.4287\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 1749.38s of the 2624.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3667\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 1745.69s of the 2621.08s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3695\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 1741.78s of the 2617.17s of remaining time.\n",
      "\t-0.3838\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.74s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 1740.82s of the 2616.21s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3656\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.35s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 1717.67s of the 2593.07s of remaining time.\n",
      "\t-0.3876\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1717.03s of the 2592.42s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.369\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.98s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1705.23s of the 2580.63s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.05%)\n",
      "\t-0.3697\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.79s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1701.37s of the 2576.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4162\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.27s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1688.05s of the 2563.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.08%)\n",
      "\t-0.3767\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 1683.08s of the 2558.47s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3654\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 1678.2s of the 2553.59s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.428\t = Validation score   (-root_mean_squared_error)\n",
      "\t24.02s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: LightGBM_r131_BAG_L1 ... Training model for up to 1651.17s of the 2526.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.04%)\n",
      "\t-0.3693\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r191_BAG_L1 ... Training model for up to 1645.96s of the 2521.36s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.369\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.33s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: CatBoost_r9_BAG_L1 ... Training model for up to 1617.32s of the 2492.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.16%)\n",
      "\t-0.3676\t = Validation score   (-root_mean_squared_error)\n",
      "\t19.94s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBM_r96_BAG_L1 ... Training model for up to 1594.25s of the 2469.64s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3663\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r22_BAG_L1 ... Training model for up to 1589.92s of the 2465.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4318\t = Validation score   (-root_mean_squared_error)\n",
      "\t36.26s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_r33_BAG_L1 ... Training model for up to 1550.43s of the 2425.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.30%)\n",
      "\t-0.3773\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r42_BAG_L1 ... Training model for up to 1545.32s of the 2420.72s of remaining time.\n",
      "\t-0.386\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: CatBoost_r137_BAG_L1 ... Training model for up to 1544.49s of the 2419.88s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3653\t = Validation score   (-root_mean_squared_error)\n",
      "\t9.42s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r102_BAG_L1 ... Training model for up to 1532.21s of the 2407.6s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3688\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.7s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost_r13_BAG_L1 ... Training model for up to 1524.6s of the 2400.0s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.17%)\n",
      "\t-0.3664\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: RandomForest_r195_BAG_L1 ... Training model for up to 1518.12s of the 2393.51s of remaining time.\n",
      "\t-0.3823\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: LightGBM_r188_BAG_L1 ... Training model for up to 1517.06s of the 2392.46s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.08%)\n",
      "\t-0.3712\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r145_BAG_L1 ... Training model for up to 1512.44s of the 2387.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3684\t = Validation score   (-root_mean_squared_error)\n",
      "\t21.25s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: XGBoost_r89_BAG_L1 ... Training model for up to 1488.14s of the 2363.54s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.05%)\n",
      "\t-0.3663\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r30_BAG_L1 ... Training model for up to 1483.74s of the 2359.14s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4719\t = Validation score   (-root_mean_squared_error)\n",
      "\t50.75s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: LightGBM_r130_BAG_L1 ... Training model for up to 1429.29s of the 2304.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.06%)\n",
      "\t-0.3677\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r86_BAG_L1 ... Training model for up to 1424.39s of the 2299.79s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.4288\t = Validation score   (-root_mean_squared_error)\n",
      "\t70.51s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: CatBoost_r50_BAG_L1 ... Training model for up to 1350.38s of the 2225.77s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.02%)\n",
      "\t-0.3652\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_r11_BAG_L1 ... Training model for up to 1339.06s of the 2214.45s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.01%)\n",
      "\t-0.3664\t = Validation score   (-root_mean_squared_error)\n",
      "\t28.69s\t = Training   runtime\n",
      "\t0.36s\t = Validation runtime\n",
      "Fitting model: XGBoost_r194_BAG_L1 ... Training model for up to 1307.39s of the 2182.78s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.09%)\n",
      "\t-0.3728\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: ExtraTrees_r172_BAG_L1 ... Training model for up to 1300.89s of the 2176.28s of remaining time.\n",
      "\t-0.3723\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.85s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: CatBoost_r69_BAG_L1 ... Training model for up to 1299.77s of the 2175.17s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=4, gpus=0, memory=0.03%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m      6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m TabularDataset(minimal_numerize_csv())\n\u001b[1;32m----> 7\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mTabularPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDiscontinued\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhigh_quality\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39mgargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1109\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_stack_levels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_stack_levels\n\u001b[0;32m   1107\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time_limit\n\u001b[1;32m-> 1109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1115\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[1;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learner\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_fit_kwargs)\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:128\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_metric \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39meval_metric\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m--> 128\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    129\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    130\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    131\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    132\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    133\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    134\u001b[0m     holdout_frac\u001b[38;5;241m=\u001b[39mholdout_frac,\n\u001b[0;32m    135\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit_trainer,\n\u001b[0;32m    136\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    137\u001b[0m     infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    138\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_fit_kwargs,\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_trainer(trainer\u001b[38;5;241m=\u001b[39mtrainer)\n\u001b[0;32m    142\u001b[0m time_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:125\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m log_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, log_str)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_multi_and_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_stack_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_stack_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2503\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2501\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rows_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_val)\n\u001b[0;32m   2502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_cols_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m-> 2503\u001b[0m model_names_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_multi_levels(\n\u001b[0;32m   2504\u001b[0m     X,\n\u001b[0;32m   2505\u001b[0m     y,\n\u001b[0;32m   2506\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[0;32m   2507\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m   2508\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m   2509\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m   2510\u001b[0m     level_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   2511\u001b[0m     level_end\u001b[38;5;241m=\u001b[39mnum_stack_levels \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   2512\u001b[0m     time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2513\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2514\u001b[0m )\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_names()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2516\u001b[0m     \u001b[38;5;66;03m# TODO v1.0: Add toggle to raise exception if no models trained\u001b[39;00m\n\u001b[0;32m   2517\u001b[0m     logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: AutoGluon did not successfully train any models\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:388\u001b[0m, in \u001b[0;36mAbstractTrainer.train_multi_levels\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, base_model_names, core_kwargs, aux_kwargs, level_start, level_end, time_limit, name_suffix, relative_stack, level_time_modifier, infer_limit, infer_limit_batch_size)\u001b[0m\n\u001b[0;32m    386\u001b[0m         core_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_core)\n\u001b[0;32m    387\u001b[0m         aux_kwargs_level[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs_level\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_limit_aux)\n\u001b[1;32m--> 388\u001b[0m     base_model_names, aux_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_new_level\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_model_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcore_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcore_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43maux_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maux_kwargs_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_suffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_limit_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_full_weighted_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m     model_names_fit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m base_model_names \u001b[38;5;241m+\u001b[39m aux_models\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_best \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_names_fit) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:536\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, core_kwargs, aux_kwargs, name_suffix, infer_limit, infer_limit_batch_size, full_weighted_ensemble, additional_full_weighted_ensemble)\u001b[0m\n\u001b[0;32m    534\u001b[0m     core_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m core_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[0;32m    535\u001b[0m     aux_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m aux_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m name_suffix\n\u001b[1;32m--> 536\u001b[0m core_models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack_new_level_core(\n\u001b[0;32m    537\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    538\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    539\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    540\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    541\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    542\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m    543\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    544\u001b[0m     infer_limit\u001b[38;5;241m=\u001b[39minfer_limit,\n\u001b[0;32m    545\u001b[0m     infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    546\u001b[0m     base_model_names\u001b[38;5;241m=\u001b[39mbase_model_names,\n\u001b[0;32m    547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcore_kwargs,\n\u001b[0;32m    548\u001b[0m )\n\u001b[0;32m    550\u001b[0m aux_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_weighted_ensemble:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:666\u001b[0m, in \u001b[0;36mAbstractTrainer.stack_new_level_core\u001b[1;34m(self, X, y, models, X_val, y_val, X_unlabeled, level, base_model_names, stack_name, ag_args, ag_args_fit, ag_args_ensemble, included_model_types, excluded_model_types, ensemble_type, name_suffix, get_models_func, refit_full, infer_limit, infer_limit_batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    663\u001b[0m fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n\u001b[0;32m    665\u001b[0m \u001b[38;5;66;03m# FIXME: TODO: v0.1 X_unlabeled isn't cached so it won't be available during refit_full or fit_extra.\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi(\n\u001b[0;32m    667\u001b[0m     X\u001b[38;5;241m=\u001b[39mX_init,\n\u001b[0;32m    668\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    669\u001b[0m     X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m    670\u001b[0m     y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m    671\u001b[0m     X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m    672\u001b[0m     models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m    673\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m    674\u001b[0m     stack_name\u001b[38;5;241m=\u001b[39mstack_name,\n\u001b[0;32m    675\u001b[0m     compute_score\u001b[38;5;241m=\u001b[39mcompute_score,\n\u001b[0;32m    676\u001b[0m     fit_kwargs\u001b[38;5;241m=\u001b[39mfit_kwargs,\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    678\u001b[0m )\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2453\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi\u001b[1;34m(self, X, y, models, hyperparameter_tune_kwargs, feature_prune_kwargs, k_fold, n_repeats, n_repeat_start, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m   2451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_repeat_start \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2452\u001b[0m     time_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 2453\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_initial(\n\u001b[0;32m   2454\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   2455\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2456\u001b[0m         models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m   2457\u001b[0m         k_fold\u001b[38;5;241m=\u001b[39mk_fold,\n\u001b[0;32m   2458\u001b[0m         n_repeats\u001b[38;5;241m=\u001b[39mn_repeats_initial,\n\u001b[0;32m   2459\u001b[0m         hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   2460\u001b[0m         feature_prune_kwargs\u001b[38;5;241m=\u001b[39mfeature_prune_kwargs,\n\u001b[0;32m   2461\u001b[0m         time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2462\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2463\u001b[0m     )\n\u001b[0;32m   2464\u001b[0m     n_repeat_start \u001b[38;5;241m=\u001b[39m n_repeats_initial\n\u001b[0;32m   2465\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2302\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_initial\u001b[1;34m(self, X, y, models, k_fold, n_repeats, hyperparameter_tune_kwargs, time_limit, feature_prune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2301\u001b[0m     time_ratio \u001b[38;5;241m=\u001b[39m hpo_time_ratio \u001b[38;5;28;01mif\u001b[39;00m hpo_enabled \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2302\u001b[0m     models \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_multi_fold(\n\u001b[0;32m   2303\u001b[0m         models\u001b[38;5;241m=\u001b[39mmodels,\n\u001b[0;32m   2304\u001b[0m         hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs,\n\u001b[0;32m   2305\u001b[0m         k_fold_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2306\u001b[0m         k_fold_end\u001b[38;5;241m=\u001b[39mk_fold,\n\u001b[0;32m   2307\u001b[0m         n_repeats\u001b[38;5;241m=\u001b[39mn_repeats,\n\u001b[0;32m   2308\u001b[0m         n_repeat_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2309\u001b[0m         time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[0;32m   2310\u001b[0m         time_split\u001b[38;5;241m=\u001b[39mtime_split,\n\u001b[0;32m   2311\u001b[0m         time_ratio\u001b[38;5;241m=\u001b[39mtime_ratio,\n\u001b[0;32m   2312\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args,\n\u001b[0;32m   2313\u001b[0m     )\n\u001b[0;32m   2315\u001b[0m multi_fold_time_elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m multi_fold_time_start\n\u001b[0;32m   2316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2410\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_fold\u001b[1;34m(self, X, y, models, time_limit, time_split, time_ratio, hyperparameter_tune_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   2408\u001b[0m         time_start_model \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   2409\u001b[0m         time_left \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time_start_model \u001b[38;5;241m-\u001b[39m time_start)\n\u001b[1;32m-> 2410\u001b[0m model_name_trained_lst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single_full(\n\u001b[0;32m   2411\u001b[0m     X, y, model, time_limit\u001b[38;5;241m=\u001b[39mtime_left, hyperparameter_tune_kwargs\u001b[38;5;241m=\u001b[39mhyperparameter_tune_kwargs_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   2412\u001b[0m )\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2183\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single_full\u001b[1;34m(self, X, y, model, X_unlabeled, X_val, y_val, X_pseudo, y_pseudo, feature_prune, hyperparameter_tune_kwargs, stack_name, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, level, time_limit, fit_kwargs, compute_score, total_resources, **kwargs)\u001b[0m\n\u001b[0;32m   2179\u001b[0m         bagged_model_fit_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_bagged_model_fit_kwargs(\n\u001b[0;32m   2180\u001b[0m             k_fold\u001b[38;5;241m=\u001b[39mk_fold, k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start, k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end, n_repeats\u001b[38;5;241m=\u001b[39mn_repeats, n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start\n\u001b[0;32m   2181\u001b[0m         )\n\u001b[0;32m   2182\u001b[0m         model_fit_kwargs\u001b[38;5;241m.\u001b[39mupdate(bagged_model_fit_kwargs)\n\u001b[1;32m-> 2183\u001b[0m     model_names_trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_and_save(\n\u001b[0;32m   2184\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   2185\u001b[0m         y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m   2186\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   2187\u001b[0m         X_val\u001b[38;5;241m=\u001b[39mX_val,\n\u001b[0;32m   2188\u001b[0m         y_val\u001b[38;5;241m=\u001b[39my_val,\n\u001b[0;32m   2189\u001b[0m         X_unlabeled\u001b[38;5;241m=\u001b[39mX_unlabeled,\n\u001b[0;32m   2190\u001b[0m         stack_name\u001b[38;5;241m=\u001b[39mstack_name,\n\u001b[0;32m   2191\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   2192\u001b[0m         compute_score\u001b[38;5;241m=\u001b[39mcompute_score,\n\u001b[0;32m   2193\u001b[0m         total_resources\u001b[38;5;241m=\u001b[39mtotal_resources,\n\u001b[0;32m   2194\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs,\n\u001b[0;32m   2195\u001b[0m     )\n\u001b[0;32m   2196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   2197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_names_trained\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1817\u001b[0m, in \u001b[0;36mAbstractTrainer._train_and_save\u001b[1;34m(self, X, y, model, X_val, y_val, stack_name, level, compute_score, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1815\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X_w_pseudo, y_w_pseudo, model, X_val, y_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_single(X, y, model, X_val, y_val, total_resources\u001b[38;5;241m=\u001b[39mtotal_resources, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1819\u001b[0m fit_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_evaluation:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:1763\u001b[0m, in \u001b[0;36mAbstractTrainer._train_single\u001b[1;34m(self, X, y, model, X_val, y_val, total_resources, **model_fit_kwargs)\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, model: AbstractModel, X_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, total_resources\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AbstractModel:\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m \u001b[38;5;124;03m    Trains model but does not add the trained model to this Trainer.\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[38;5;124;03m    Returns trained model object.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1763\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, X_val\u001b[38;5;241m=\u001b[39mX_val, y_val\u001b[38;5;241m=\u001b[39my_val, total_resources\u001b[38;5;241m=\u001b[39mtotal_resources, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_fit_kwargs)\n\u001b[0;32m   1764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py:854\u001b[0m, in \u001b[0;36mAbstractModel.fit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_fit_resources(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_memory_usage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 854\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    856\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\stacker_ensemble_model.py:165\u001b[0m, in \u001b[0;36mStackerEnsembleModel._fit\u001b[1;34m(self, X, y, compute_base_preds, time_limit, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     time_limit \u001b[38;5;241m=\u001b[39m time_limit \u001b[38;5;241m-\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_fit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, time_limit\u001b[38;5;241m=\u001b[39mtime_limit, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:273\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit\u001b[1;34m(self, X, y, X_val, y_val, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, groups, _skip_oof, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;66;03m# Reserve time for final refit model\u001b[39;00m\n\u001b[0;32m    272\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m folds_to_fit \u001b[38;5;241m/\u001b[39m (folds_to_fit \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.2\u001b[39m)\n\u001b[1;32m--> 273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_folds(\n\u001b[0;32m    274\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    275\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    276\u001b[0m     model_base\u001b[38;5;241m=\u001b[39mmodel_base,\n\u001b[0;32m    277\u001b[0m     X_pseudo\u001b[38;5;241m=\u001b[39mX_pseudo,\n\u001b[0;32m    278\u001b[0m     y_pseudo\u001b[38;5;241m=\u001b[39my_pseudo,\n\u001b[0;32m    279\u001b[0m     k_fold\u001b[38;5;241m=\u001b[39mk_fold,\n\u001b[0;32m    280\u001b[0m     k_fold_start\u001b[38;5;241m=\u001b[39mk_fold_start,\n\u001b[0;32m    281\u001b[0m     k_fold_end\u001b[38;5;241m=\u001b[39mk_fold_end,\n\u001b[0;32m    282\u001b[0m     n_repeats\u001b[38;5;241m=\u001b[39mn_repeats,\n\u001b[0;32m    283\u001b[0m     n_repeat_start\u001b[38;5;241m=\u001b[39mn_repeat_start,\n\u001b[0;32m    284\u001b[0m     save_folds\u001b[38;5;241m=\u001b[39msave_bag_folds,\n\u001b[0;32m    285\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    287\u001b[0m )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# FIXME: Cleanup self\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# FIXME: Support `can_refit_full=False` models\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m refit_folds:\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\bagged_ensemble_model.py:689\u001b[0m, in \u001b[0;36mBaggedEnsembleModel._fit_folds\u001b[1;34m(self, X, y, model_base, X_pseudo, y_pseudo, k_fold, k_fold_start, k_fold_end, n_repeats, n_repeat_start, time_limit, sample_weight, save_folds, groups, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_fit_args \u001b[38;5;129;01min\u001b[39;00m fold_fit_args_list:\n\u001b[0;32m    688\u001b[0m     fold_fitting_strategy\u001b[38;5;241m.\u001b[39mschedule_fold_model_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfold_fit_args)\n\u001b[1;32m--> 689\u001b[0m \u001b[43mfold_fitting_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_all_folds_scheduled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# No need to add child times or save child here as this already occurred in the fold_fitting_strategy\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_child(model\u001b[38;5;241m=\u001b[39mmodel, add_child_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:661\u001b[0m, in \u001b[0;36mParallelFoldFittingStrategy.after_all_folds_scheduled\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 661\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_pseudo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pseudo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_base_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_node_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\autogluon\\core\\models\\ensemble\\fold_fitting_strategy.py:600\u001b[0m, in \u001b[0;36mParallelFoldFittingStrategy._run_parallel\u001b[1;34m(self, X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\u001b[0m\n\u001b[0;32m    598\u001b[0m unfinished \u001b[38;5;241m=\u001b[39m job_refs\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished:\n\u001b[1;32m--> 600\u001b[0m     finished, unfinished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43munfinished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m     finished \u001b[38;5;241m=\u001b[39m finished[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    602\u001b[0m     fold_ctx \u001b[38;5;241m=\u001b[39m job_fold_map\u001b[38;5;241m.\u001b[39mget(finished, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\ray\\_private\\auto_init_hook.py:24\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     23\u001b[0m     auto_init_ray()\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Caltech\\CS 155\\project1\\.conda\\lib\\site-packages\\ray\\_private\\worker.py:2732\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[0;32m   2730\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[0;32m   2731\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m-> 2732\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:3012\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpython\\ray\\_raylet.pyx:400\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def minimal_numerize_csv():\n",
    "    df = pd.read_csv('train.csv').drop('customerID', axis=1)\n",
    "    df['Discontinued'] = (df['Discontinued'] == 'Yes').astype(float)\n",
    "    return df\n",
    "\n",
    "train_data = TabularDataset(minimal_numerize_csv())\n",
    "predictor = TabularPredictor(label='Discontinued', problem_type='regression').fit(train_data=train_data, presets='high_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomerID\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      2\u001b[0m y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictor' is not defined"
     ]
    }
   ],
   "source": [
    "y = predictor.predict(pd.read_csv('test.csv').drop('customerID', axis=1))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "write_submission(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
